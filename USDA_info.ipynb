{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaef5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd772c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome() \n",
    "\n",
    "# Define the base URL of the website\n",
    "base_url = \"https://www.usda.gov/media/agency-reports\"\n",
    "\n",
    "# Create a list to store all link data\n",
    "all_link_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f686bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data from page 1\n",
      "Scraped data from page 2\n",
      "Scraped data from page 3\n",
      "Scraped data from page 4\n",
      "Scraped data from page 5\n",
      "Data has been exported to links_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the next 5 pages\n",
    "for page_number in range(1, 6):  # Pages 1 to 5\n",
    "    # Define the URL for the current page\n",
    "    url = f\"{base_url}?start_date=08/01/2023&end_date=09/12/2023&page={page_number}\"\n",
    "\n",
    "    # Visit the URL using Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source after it's fully loaded (Selenium waits for the page to load)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all elements with the specific class (modify class_name as needed)\n",
    "    blocks = soup.find_all('span', class_='agency-report-title')\n",
    "\n",
    "    # Iterate through the blocks on the current page\n",
    "    for block in blocks:\n",
    "        # Find all the links within the block\n",
    "        links = block.find_all('a')\n",
    "\n",
    "        # Iterate through the links within the block\n",
    "        for link in links:\n",
    "            # Get the URL of the link\n",
    "            link_url = link.get('href')\n",
    "\n",
    "            # Check if the link is valid (not None and not empty)\n",
    "            if link_url:\n",
    "                # Handle relative URLs manually by combining with the base URL\n",
    "                if not link_url.startswith(\"http\"):\n",
    "                    link_url = base_url + link_url\n",
    "\n",
    "                # Send an HTTP GET request to the full link URL\n",
    "                link_response = requests.get(link_url)\n",
    "\n",
    "                # Check if the request to the link was successful (status code 200)\n",
    "                if link_response.status_code == 200:\n",
    "                    # Parse the HTML content of the link's page\n",
    "                    link_soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "\n",
    "                    # Extract the title of the article\n",
    "                    article_title = link_soup.title.string\n",
    "\n",
    "                    # Extract the source information\n",
    "                    source_element = link_soup.find('span', class_='attribute contact_organization')\n",
    "                    source = source_element.get_text(strip=True) if source_element else \"\"\n",
    "\n",
    "                    # Append the link URL, article title, and source to the all_link_data list\n",
    "                    all_link_data.append([link_url, article_title, source])\n",
    "\n",
    "    print(f\"Scraped data from page {page_number}\")\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_file = \"links_data.csv\"\n",
    "\n",
    "# Write all the link data to a CSV file\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Link\", \"Article Title\", \"Source\"])  # Write header row\n",
    "    writer.writerows(all_link_data)  # Write all link data\n",
    "\n",
    "print(f\"Data has been exported to {csv_file}\")\n",
    "\n",
    "# Close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ffe072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title and ID</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nass.usda.gov/Publications/Calenda...</td>\n",
       "      <td>Publication | Cotton System Consumption and St...</td>\n",
       "      <td>National Agricultural Statistics Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.nass.usda.gov/Publications/Calenda...</td>\n",
       "      <td>Publication | Fats and Oils: Oilseed Crushings...</td>\n",
       "      <td>National Agricultural Statistics Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nass.usda.gov/Publications/Calenda...</td>\n",
       "      <td>Publication | Flour Milling Products | ID: cr5...</td>\n",
       "      <td>National Agricultural Statistics Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.nass.usda.gov/Publications/Calenda...</td>\n",
       "      <td>Publication | Grain Crushings and Co-Products ...</td>\n",
       "      <td>National Agricultural Statistics Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.nass.usda.gov/Publications/Calenda...</td>\n",
       "      <td>Publication | Honey Bee Colonies | ID: rn30113...</td>\n",
       "      <td>National Agricultural Statistics Service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://www.nass.usda.gov/Publications/Calenda...   \n",
       "1  https://www.nass.usda.gov/Publications/Calenda...   \n",
       "2  https://www.nass.usda.gov/Publications/Calenda...   \n",
       "3  https://www.nass.usda.gov/Publications/Calenda...   \n",
       "4  https://www.nass.usda.gov/Publications/Calenda...   \n",
       "\n",
       "                                        Title and ID  \\\n",
       "0  Publication | Cotton System Consumption and St...   \n",
       "1  Publication | Fats and Oils: Oilseed Crushings...   \n",
       "2  Publication | Flour Milling Products | ID: cr5...   \n",
       "3  Publication | Grain Crushings and Co-Products ...   \n",
       "4  Publication | Honey Bee Colonies | ID: rn30113...   \n",
       "\n",
       "                                     Source  \n",
       "0  National Agricultural Statistics Service  \n",
       "1  National Agricultural Statistics Service  \n",
       "2  National Agricultural Statistics Service  \n",
       "3  National Agricultural Statistics Service  \n",
       "4  National Agricultural Statistics Service  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_link_data)\n",
    "df = df.rename(columns={0: \"URL\", 1: \"Title and ID\", 2: \"Source\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387080ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
